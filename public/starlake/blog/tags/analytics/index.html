<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">3 posts tagged with &quot;Analytics&quot; | Declarative Data Pipelines</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://starlake.ai/starlake/blog/tags/analytics"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="3 posts tagged with &quot;Analytics&quot; | Declarative Data Pipelines"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/starlake/img/favicon_starlake.ico"><link data-rh="true" rel="alternate" href="https://starlake.ai/starlake/blog/tags/analytics" hreflang="en"><link data-rh="true" rel="alternate" href="https://starlake.ai/starlake/blog/tags/analytics" hreflang="x-default"><link data-rh="true" rel="canonical" href="https://www.linkedin.com/pulse/starlake-oss-bringing-declarative-programming-data-engineering-5fdde"><link rel="alternate" type="application/rss+xml" href="/starlake/blog/rss.xml" title="Declarative Data Pipelines RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/starlake/blog/atom.xml" title="Declarative Data Pipelines Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-207943293-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FYS72XYD48"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-FYS72XYD48",{anonymize_ip:!0})</script><link rel="stylesheet" href="/starlake/assets/css/styles.e986530a.css">
<script src="/starlake/assets/js/runtime~main.97319b8f.js" defer="defer"></script>
<script src="/starlake/assets/js/main.b5b6df85.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a href="https://starlake.ai" target="_blank" rel="noopener noreferrer" class="navbar__brand"><div class="navbar__logo"><img src="/starlake/img/starlake-logo.png" alt="Starlake" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/starlake/img/starlake-logo.png" alt="Starlake" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Starlake</b></a><a class="navbar__item navbar__link" href="/starlake/docs/intro">Documentation</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/starlake/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/starlake-ai/starlake" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link header-icon-link" aria-label="GitHub repository"></a><a href="https://join.slack.com/t/starlakeai/shared_invite/zt-28vf5d49s-rnyuh70OrJjcX_2Vz2mafw" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-slack-link header-icon-link" aria-label="Community"></a><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/starlake/blog/how-unit-test-your-sql-data-pipelines">How to unit test your data pipelines</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/starlake/blog/polars-vs-spark">Polars versus Spark</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/starlake/blog/starlake-oss">Starlake OSS - Bringing Declarative Programming to Data Engineering and Analytics</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/starlake/blog/rls-cls-big-query">Column  and Row Level Security in BigQuery</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2021</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/starlake/blog/spark-big-query-partitioning">Handling Dynamic Partitioning and Merge with Spark on BigQuery</a></li></ul></div></nav></aside><main class="col col--7"><header class="margin-bottom--xl"><h1>3 posts tagged with &quot;Analytics&quot;</h1><a href="/starlake/blog/tags">View All Tags</a></header><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/starlake/blog/how-unit-test-your-sql-data-pipelines">How to unit test your data pipelines</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-05T00:00:00.000Z">July 5, 2024</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/bounkong/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://2.gravatar.com/avatar/82db9f824c7cc18bd7b7e091c5752353c869752d8ff5bac90e8770de89635cf9" alt="Bounkong Khamphousone"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/bounkong/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Bounkong Khamphousone</span></a></div><small class="authorTitle_nd0D" title="Starlake Core Team">Starlake Core Team</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown">
<p>In today&#x27;s data-driven landscape, ensuring the reliability and accuracy of your data warehouse is paramount. The cost of not testing your data can be astronomical, leading to critical business decisions based on faulty data and eroding trust. </p>
<p>The path to rigorous data testing comes with its own set of challenges. In this article, I will highlight how you can confidently deploy your data pipelines by leveraging <a href="https://github.com/starlake-ai/jsqltranspiler" target="_blank" rel="noopener noreferrer">Starlake JSQLTranspiler</a> and DuckDB, while also reducing costs. we will go beyond testing your transform usually written in SQL and see how we can also test our Ingestion jobs.</p>
<p><img decoding="async" loading="lazy" src="/starlake/assets/images/data-pipeline-cicd-bcc87fb85edbabe13eee07d78978037c.gif" width="752" height="423" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-art-of-mastering-data-pipelines">The art of mastering data pipelines<a href="#the-art-of-mastering-data-pipelines" class="hash-link" aria-label="Direct link to The art of mastering data pipelines" title="Direct link to The art of mastering data pipelines">​</a></h2>
<p>Mastering your data pipeline is a challenging art. A data pipeline generally contains the following phases:</p>
<ul>
<li><strong>Collection:</strong> Extracting data from sources</li>
<li><strong>Ingestion:</strong> Loading the extracted data into the data warehouse</li>
<li><strong>Transformation:</strong> A phase that ultimately adds value to the collected data</li>
</ul>
<p>The table below summarizes the tests run by Starlake on <a href="https://starlake.ai/starlake/docs/next/category/load" target="_blank" rel="noopener noreferrer">Load</a> &amp; <a href="https://starlake.ai/starlake/docs/next/category/transform" target="_blank" rel="noopener noreferrer">Transform</a> jobs:</p>
<table><thead><tr><th>Check to run on</th><th>Ingestion Test</th><th>Transform Test</th></tr></thead><tbody><tr><td>Validate the filename pattern</td><td>✓</td><td></td></tr><tr><td>Validate the file structure (number and types of attributes, input file format - CSV / JSON / XML / FIXED-WIDTH)</td><td>✓</td><td></td></tr><tr><td>Check if loaded files or transform SQL SELECT statements  are materialized according to the defined strategy (APPEND / OVERWRITE / UPSERT_BY_KEY, SCD2 …)</td><td>✓</td><td>✓</td></tr><tr><td>Check for missing or unexpected records in the resulting table</td><td>✓</td><td>✓</td></tr><tr><td>Check if the resulting table has a correct schema</td><td>✓</td><td>✓</td></tr><tr><td>Check all expectations</td><td>✓</td><td>✓</td></tr><tr><td>Check time based query output with time freeze</td><td>✓</td><td>✓</td></tr><tr><td>The results of these automated tests are designed for both human review and CI/CD integration. For human review, a website is generated to help users easily identify failures and their causes. For CI/CD integration, a JUnit report is generated, and there is an option to specify a minimum coverage threshold. If the evaluated coverage falls below this threshold, the command will result in an error, and any failing tests will also trigger errors.</td><td></td><td></td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="untested-sql-costs">Untested SQL costs<a href="#untested-sql-costs" class="hash-link" aria-label="Direct link to Untested SQL costs" title="Direct link to Untested SQL costs">​</a></h2>
<p>Thanks to data pipelines unit testing, we drastically reduce the development cost. Not running tests seems to allow high project&#x27;s velocity, thereby delivering value quickly. However, the cost of a feature does not stop at its simple development but encompasses all the efforts put in until the feature&#x27;s completion. Below are some hidden costs.</p>
<ul>
<li><strong>Identifying bugs</strong>
<ul>
<li>Without unit tests, verifying development requires deploying the project. This deployment raises challenging questions about the deployment strategy and its rollback or correction procedures.</li>
<li>Verification might be carried out by a separate QA team, sometimes even outside the project team. This can lead to the use of feature flags to avoid deploying to production, complicating the implementation. Additionally, waiting for feedback from the QA team introduces delays, increasing the cost of fixing any bugs that arise.</li>
<li>Depending on the deployment strategy, verification may also be incomplete due to a lack of control over the test data used.</li>
</ul>
</li>
<li><strong>Maintenance and Evolution Complexity</strong>
<ul>
<li>Many of us have faced a massive query and struggled to make modifications without disrupting existing functionality, all while aiming for improvements like optimizing processing time. Rigorous unit tests can help with this. They allow us to enhance the expected outcomes in current datasets, create new ones, and compare the modified query results with these expectations. This significantly reduces the risk of regression.</li>
</ul>
</li>
<li><strong>Decreased productivity</strong>
<ul>
<li>The absence of automated tests often means manually re-running parts or all of the system to ensure correct integration, which can lead to spending time fixing collateral bugs and thus reducing overall productivity. As the project advances, more components need verification, making the process even more time-consuming. This significantly diminishes the willingness to refactor or revise code.</li>
</ul>
</li>
<li><strong>Promoting expertise</strong>
<ul>
<li>Without unit tests, teams often assign the same tasks to the same people, which hinders skill development and increases the risk of knowledge loss due to turnover.</li>
</ul>
</li>
<li><strong>Customer dissatisfaction</strong>
<ul>
<li>A project with uncertain product output quality often leads to dissatisfaction, frustration, and a loss of trust in the individual, the team, or the product.</li>
</ul>
</li>
</ul>
<p>We are all aware of the hidden costs associated with the absence of tests; in my opinion, these are the most significant. Therefore, we will explore how to manage a data pipeline.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="writing-unit-test-in-starlake">Writing unit test in Starlake<a href="#writing-unit-test-in-starlake" class="hash-link" aria-label="Direct link to Writing unit test in Starlake" title="Direct link to Writing unit test in Starlake">​</a></h2>
<p>Suppose we have the following transform.
<img decoding="async" loading="lazy" alt="Starlake transform folder hierachy" src="/starlake/assets/images/starlake-transform-folder-hierarchy-9713252e3cd60761bfdd8c239b26bca1.png" title="Starlake transform folder hierachy" width="744" height="347" class="img_ev3q"></p>
<p>We test it by creating the following hierarchy:
<img decoding="async" loading="lazy" alt="Starlake test transform folder hierachy" src="/starlake/assets/images/starlake-test-transform-folder-hierarchy-aaed9899b6fa836991e59dc834b2fe75.png" title="Starlake test transform folder hierachy" width="1488" height="964" class="img_ev3q"></p>
<p>This is then how it is executed
<img decoding="async" loading="lazy" alt="Data pipeline unit test lifecycle" src="/starlake/assets/images/data-pipeline-unit-test-lifecycle-3f116018de74957f7a1fd9a31d6b4603.gif" title="Data pipeline unit test lifecycle" width="1488" height="837" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="starlake-unit-tests-benefits">Starlake unit tests benefits<a href="#starlake-unit-tests-benefits" class="hash-link" aria-label="Direct link to Starlake unit tests benefits" title="Direct link to Starlake unit tests benefits">​</a></h2>
<p>Running tests on a local DuckDB database instead of the target Data Warehouse has the following advantages:</p>
<ul>
<li><strong>Fast Feedback:</strong> Local execution is significantly faster than using a remote database due to network latency. Additionally, the local environment might be better suited for handling small volumes of test data.</li>
<li><strong>No Execution Cost:</strong> Depending on the pricing model of the target database, creating temporary resources and executing queries can incur both execution and storage costs.</li>
<li><strong>Setup and Cleanup of Automated Tests:</strong> Guarantee of resource isolation.</li>
<li><strong>Credential Issues:</strong> Running tests against a target database requires credentials, which may pose security risks.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>In this article, we have demonstrated how adopting unit testing, a crucial practice for software engineers, can significantly enhance the quality of our data pipelines. This approach not only reduces overall costs in the medium to long term but also ensures the maintenance of dynamic and enduring documentation. Additionally, implementing unit tests is essential for rigorous CI/CD processes, enabling seamless continuous data pipeline deployment.</p>
<p>If you encounter any issues while performing your tests locally, please report them on the <a href="https://github.com/starlake-ai/starlake" target="_blank" rel="noopener noreferrer">Starlake GitHub repository</a>. Your feedback is invaluable in improving local test coverage, empowering more data engineers to deploy their work confidently and smoothly. For further discussions and support, join our team on <a href="https://join.slack.com/t/starlakeai/shared_invite/zt-28vf5d49s-rnyuh70OrJjcX_2Vz2mafw" target="_blank" rel="noopener noreferrer">Slack</a>.</p>
<p>We greatly appreciate your contributions. If you found this article helpful, please star the project on GitHub and share it on your social networks to help us reach a broader audience.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/unit-test">Unit Test</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/databricks">Databricks</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/duck-db">DuckDB</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/data-engineering">Data Engineering</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/analytics">Analytics</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/big-query">BigQuery</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/snowflake">Snowflake</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/redshift">Redshift</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/starlake/blog/polars-vs-spark">Polars versus Spark</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-05-28T00:00:00.000Z">May 28, 2024</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/hayssams/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png" alt="Hayssam Saleh"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/hayssams/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Hayssam Saleh</span></a></div><small class="authorTitle_nd0D" title="Starlake Core Team">Starlake Core Team</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>Polars is often compared to Spark. In this post, I will highlight the main differences and the best use cases for each in my data engineering activities.</p>
<p>As a Data Engineer, I primarily focus on the following goals:</p>
<ol>
<li>Parsing files, validating their input, and loading the data into the target data warehouse.</li>
<li>Once the data is loaded, applying transformations by joining and aggregating the data to build KPIs.</li>
</ol>
<p>However, on a daily basis, I also need to develop on my laptop and test my work locally before delivering it to the CI pipeline and then to production.</p>
<p>What about my fellow data scientist colleagues? They need to run their workload on production data through their favorite notebook environment.</p>
<p>This post addresses the following points:</p>
<ul>
<li>How suitable each tool is for loading files into your data warehouse.</li>
<li>How easy and powerful each tool is for performing transformations.</li>
<li>How easy it is to test your code locally before deploying to the cloud</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="load-tasks">Load tasks<a href="#load-tasks" class="hash-link" aria-label="Direct link to Load tasks" title="Direct link to Load tasks">​</a></h2>
<p>Data loading seems easy at first glance, as almost all databases and APIs offer some sort of single-line command to load a few lines or millions of records quickly. However, this simplicity disappears when you encounter real-world cases such as:</p>
<ul>
<li>Fixed-width fields: These files are typically exported from mainframe databases.</li>
<li>XML files: I sometimes work in  the finance industry where SWIFT is a common XML file format that will be around for some time.</li>
<li>Multi-character CSV: For example, where the separator consists of two characters like ||.</li>
<li>File validation: You cannot trust the files you receive and need to check their content thoroughly.</li>
</ul>
<p>Loading correct CSV or JSON files using Spark or Polars is straightforward. However, it is also straightforward using your favorite database command line utility, making this capabilities somewhat redundant and even slower than the native data warehouse load feature.</p>
<p>However, in real-world scenarios, you want to ensure the incoming file adheres to the expected schema and load a variety of file formats not supported natively. This is where Spark excels compared to Polars, as it allows for the parallel loading of your JSONL or CSV files and offers through map operations local and distributed validation of your incoming file.</p>
<p>As XML and multichar and multiline CSV are only supported by Spark, dealing with file parsing for data loading, Spark is definitely the most suitable solution.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transform-tasks">Transform tasks<a href="#transform-tasks" class="hash-link" aria-label="Direct link to Transform tasks" title="Direct link to Transform tasks">​</a></h2>
<p>Three interfaces are provided to transform data: SQL, Dafarames and Datasets.</p>
<p><strong>SQL</strong></p>
<p>SQL is the preferred language for most data analysts when it comes to computing aggregations. One of its key benefits is its widespread understanding, which eliminates the learning curve.</p>
<p>You can use SQL with both Spark and Polars, but Polars has significant limitations. It does not support SQL Window functions nor does it support the SQL statements for update, insert, delete, or merge operations.</p>
<p><strong>Dataframes</strong></p>
<p>Both Polars and Spark offer excellent support for DataFrames. The main added value of using DataFrames is the ability to reuse portions of code and access features not available in standard SQL, such as lambda functions, JSON handling and array manipulation.</p>
<p><strong>Datasets</strong></p>
<p>As a software engineer, I particularly appreciate Datasets. Datasets are typed DataFrames, meaning that syntax, column names, and types are checked at compile time. If you believe that statically typed languages greatly enhance code quality, then you understand the added value of datasets.</p>
<p>Datasets are only supported by Spark, allowing you to write clean, reusable, and statically typed transformations. They are available exclusively to statically typed languages such as Java or Scala.</p>
<p>Spark stands out as the only tool with complete support for SQL, DataFrames, and Datasets. Polars’ limited support for SQL makes it less suitable for my data engineering tasks.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cloud-era">Cloud Era<a href="#cloud-era" class="hash-link" aria-label="Direct link to Cloud Era" title="Direct link to Cloud Era">​</a></h2>
<p>At least 60% of the projects I have worked on are cloud-based, primarily targeting Amazon Redshift, Google BigQuery, Databricks, Snowflake, or Azure Synapse. All of these platforms offer serverless support for Spark, making it incredibly easy to run workloads by simply providing your PySpark script and letting the cloud provider handle the rest.</p>
<p>In the cloud environment, Spark is definitely the tool of choice as I see it.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="single-node">Single node<a href="#single-node" class="hash-link" aria-label="Direct link to Single node" title="Direct link to Single node">​</a></h2>
<p>There has been much discussion about Spark being slow or too heavy for single-node computers. I was particularly interested in running this test since I currently execute most of my workloads on a single-node Google Cloud Run job with Spark embedded in my Docker image.</p>
<p>I decided to conduct this test on my almost 3-year-old MacBook Pro M1 Max with 64GB of memory. The test involved loading 27GB of CSV data, selecting a few attributes, computing metrics on those selected attributes, and then saving the results to a Parquet file.</p>
<p>I ran Spark with default settings and without any fine-tuning. This means it utilized all 10 cores of my MacBook Pro M1 Max but only 1 gigabyte of memory for execution.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>I could have optimized my Spark workload, but given the small size of the dataset (27GB of CSV), it didn&#x27;t make sense. The default settings were sufficient.</p></div></div>
<p>Here are the results after a cold restart  of my laptop before each test  to ensure the test did not benefit from any operating system cache.</p>
<ul>
<li>
<p>Apache Spark pipeline took: 29 seconds</p>
</li>
<li>
<p>Polars pipeline took: 56 seconds</p>
</li>
</ul>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Rerunning Polars gave me 23 seconds instead of 56 seconds. This discrepancy is probably due to filesystem caching by the operating system.</p></div></div>
<p>Load and Save Test: Load a 27GB CSV file with 193 columns per record and save  the result as parquet.</p>
<ul>
<li>
<p>Apache Spark pipeline took: 2mn 18s</p>
</li>
<li>
<p>Polars pipeline took: 2mn 32s</p>
</li>
</ul>
<p>Load parquet and filter on column value then return count : Load a 74 millions records parquet file with 193 columns, filter on &#x27;model&#x27; column and return count.</p>
<ul>
<li>
<p>Apache Spark pipeline took: 3 seconds</p>
</li>
<li>
<p>Polars pipeline took: 28 seconds</p>
</li>
</ul>
<p>The table below summarises the results</p>
<table><thead><tr><th>Task</th><th>Spark 1GB memory</th><th>Polars All the available memory</th></tr></thead><tbody><tr><td>Load CSV, aggregate and save the aggregation as parquet</td><td>29s</td><td>56s</td></tr><tr><td>Load CSV and Save parquet</td><td>2mn 18s</td><td>2mn 32s</td></tr><tr><td>Load Parquet, filter and count</td><td>3s</td><td>28s</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>I don’t think it is time to switch from Spark to Polars, at least for those of us accustomed to the JVM, running workloads in the cloud, or even working on small datasets. However, Polars may be a perfect fit for those familiar with pandas.</p>
<p>As of today, Spark is the only framework I see that can handle both local and distributed workloads, adapt to on-premise and cloud serverless jobs, and provide the complete SQL support required for most of our transformations.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/polars">Polars</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/spark">Spark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/databricks">Databricks</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/data-engineering">Data Engineering</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/analytics">Analytics</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/starlake/blog/starlake-oss">Starlake OSS - Bringing Declarative Programming to Data Engineering and Analytics</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-05-14T00:00:00.000Z">May 14, 2024</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/hayssams/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png" alt="Hayssam Saleh"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/hayssams/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Hayssam Saleh</span></a></div><small class="authorTitle_nd0D" title="Starlake Core Team">Starlake Core Team</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>The advent of declarative programming through tools like Ansible and Terraform, has revolutionized infrastructure deployment by allowing developers to achieve intended goals without specifying the order of code execution.</p>
<p>This paradigm shift brings forth benefits such as reduced error rates, significantly shortened development cycles, enhanced code readability, and increased accessibility for developers of all levels.</p>
<p>This is the story of how a small team of developers crafted a platform that goes beyond the boundaries  of conventional data engineering by applying a declarative approach to data extraction, loading, transformation and orchestration.</p>
<p><img decoding="async" loading="lazy" alt="Starlake" src="/starlake/assets/images/starlake-draw-87c6585ebff68be91f9ab04a67d5df0d.png" width="936" height="600" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-genesis">The Genesis<a href="#the-genesis" class="hash-link" aria-label="Direct link to The Genesis" title="Direct link to The Genesis">​</a></h2>
<p>Back in 2015, at the helm of ebiznext, a boutique data engineering company, we faced a daunting challenge. Our client, a prominent entity in need of a robust big data solution, sought to harness the power of Hadoop and Spark. Despite our modest size (20 people), we dared to compete against industry giants with tenfold resources (100.000+ headcount).</p>
<p>Our only chance to succeed was to innovate: we needed a data platform that could exponentially outperform the traditional ETL solutions pushed by our competitors. To build data pipelines these GUI based ETLs  require an effort that is proportional to the number and complexity of the sources.</p>
<p>Determined to disrupt this norm, we embarked on a quest to devise a DevOps friendly platform capable of lightning-fast data ingestion from any source, without the drawbacks of ETLs or specialized engineering skills.</p>
<p>The day of the tender, our ability to deliver a solution that could load data in a few weeks instead of many months allowed us to stand out from the competition and win the project.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="expisode-1-smartlake-emerges">Expisode 1: Smartlake Emerges<a href="#expisode-1-smartlake-emerges" class="hash-link" aria-label="Direct link to Expisode 1: Smartlake Emerges" title="Direct link to Expisode 1: Smartlake Emerges">​</a></h2>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>The basic idea behind Smartlake was that no datawarehouse would stay clean if data quality is checked after the data has been loaded and this pre-load quality checks needed to be handled by data owners.</p></div></div>
<p>This left us with little choice but to embrace the declarative approach. Empowering business users, we devised a system where data formats and transformations could be described in simple JSON files. Smartlake wasn’t merely a code generator; it was a versatile engine, seamlessly ingesting diverse data formats, executing transformations, and orchestrating operations with unparalleled efficiency.</p>
<p>To streamline user interaction, we devised an intuitive Excel-to-JSON converter, enabling effortless specification of input formats. Thanks to Smartlake and its declarative approach, the business users were able to define load and transformation operations in a matter of minutes.</p>
<p><strong>Smartlake Standout features</strong></p>
<ul>
<li>
<p>Load almost any file format at Spark speed (CSV, JSON, XML, FIXED WITH, Multi-record types …) or Kafka topic</p>
</li>
<li>
<p>Validate fields using user-defined schemas with user defined semantic types</p>
</li>
<li>
<p>Apply transformations on the fly to data being loaded (GDPR, normalisation, computed fields ...) with and without schema evolution</p>
</li>
<li>
<p>Sink to almost any target including Spark, Kafka, Elasticsearch.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="episode-2-evolution-to-starlake">Episode 2: Evolution to Starlake<a href="#episode-2-evolution-to-starlake" class="hash-link" aria-label="Direct link to Episode 2: Evolution to Starlake" title="Direct link to Episode 2: Evolution to Starlake">​</a></h2>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>The basic idea behind Starlake was to bring in all Smartlake benefits to the cloud by leveraging serverless services and Cloud Datawarehouses capabilities while minimising development and execution costs.</p></div></div>
<p>As the data landscape evolved, so did our vision. Cloud data warehouses emerged as formidable competitors to Spark for query execution. Recognizing this shift, we evolved Smartlake into Starlake, preserving its declarative essence while embracing YAML for enhanced readability. We maintained Spark’s prowess to run inside single or multiple container(s) for data ingestion, leveraging cloud data warehouses for query execution.</p>
<p>This strategic blend allowed us to optimize performance and cost-effectiveness based on specific workload requirements. The result was a reimagined platform, tailored for the cloud era, yet grounded in the principles of efficiency and simplicity that defined its inception.</p>
<p>The result is the Starlake OSS project that you can find on <a href="https://github.com/starlake-ai/starlake" target="_blank" rel="noopener noreferrer">Github</a>.</p>
<p>The capabilities of Starlake are extensively described <a href="https://github.com/starlake-ai/starlake" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-people-behind-starlake">The people behind Starlake<a href="#the-people-behind-starlake" class="hash-link" aria-label="Direct link to The people behind Starlake" title="Direct link to The people behind Starlake">​</a></h2>
<p>Smartlake, the precursor to Starlake, owes its existence to the collective efforts of numerous individuals, but a select few stand out for their exceptional contributions:</p>
<ul>
<li>
<p><a href="https://www.linkedin.com/in/samklr/?lipi=urn%3Ali%3Apage%3Ad_flagship3_publishing_post_edit%3BErFcjpiuROmx%2F%2BQ9qmKozQ%3D%3D" target="_blank" rel="noopener noreferrer">Sam Bessalah</a> With Sam’s presence, rallying others became effortless. His visionary outlook and knack for simplifying complexities proved transformative, setting a new standard for implementation.</p>
</li>
<li>
<p><a href="https://www.linkedin.com/in/oliviergirardot/?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3By9rNw1mSTEqx928hK46n9w%3D%3D" target="_blank" rel="noopener noreferrer">Olivier Girardot</a> Every team has its coding wizard, and Olivier filled that role impeccably. From leveraging Spark codegen to exploring mathematical frameworks like matryoshka, he pushed the boundaries, mentoring the team with his expertise spanning Docker, Ansible, Python, Scala and Spark internals.</p>
</li>
<li>
<p><a href="https://www.linkedin.com/in/valentin-kasas-937a5837/?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3By9rNw1mSTEqx928hK46n9w%3D%3D" target="_blank" rel="noopener noreferrer">Valentin Kasas</a> Valentin championed functional programming in Scala. Introducing concepts like recursion schemes, he empowered the team to craft code that was not just functional but also elegant and maintainable.</p>
</li>
</ul>
<p>As the journey progressed towards the cloud, long time data experts joined and made Starlake what it is today:</p>
<ul>
<li>
<p><a href="https://www.linkedin.com/in/bounkong/" target="_blank" rel="noopener noreferrer">Bounkong Khamphousone</a> The speed and efficiency of Starlake’s extraction and load engines owe much to his contributions.</p>
</li>
<li>
<p><a href="https://www.linkedin.com/in/mohamad-kassir-%F0%9F%93%8A%E2%98%81%EF%B8%8F-399b562b/" target="_blank" rel="noopener noreferrer">Mohamad Kassir</a> His direct involvement with customer projects and his in-depth knowledge of cloud platforms and business needs have been major assets in the evolution of Starlake.</p>
</li>
<li>
<p><a href="https://www.linkedin.com/in/elarib/?lipi=urn%3Ali%3Apage%3Ad_flagship3_publishing_post_edit%3BErFcjpiuROmx%2F%2BQ9qmKozQ%3D%3D" target="_blank" rel="noopener noreferrer">Abdelhamide EL ARIB</a> An early contributor to the load engine, this foresight and execution prowess played a significant role in shaping the platform’s today capabilities.</p>
</li>
<li>
<p><a href="https://www.linkedin.com/in/smanciot/?lipi=urn%3Ali%3Apage%3Ad_flagship3_publishing_post_edit%3BErFcjpiuROmx%2F%2BQ9qmKozQ%3D%3D" target="_blank" rel="noopener noreferrer">Stephane Manciot</a> The developer behind Starlake’s declarative workflows on top of Airflow and Dagster, was pivotal in shaping its operational backbone.</p>
</li>
<li>
<p><a href="https://www.linkedin.com/in/cyrille-ch%C3%A9p%C3%A9lov/?lipi=urn%3Ali%3Apage%3Ad_flagship3_publishing_post_edit%3BErFcjpiuROmx%2F%2BQ9qmKozQ%3D%3D" target="_blank" rel="noopener noreferrer">Cyrille Chépélov</a> A master of codebase optimisation, Cyrille’s rewrite efforts were instrumental in ensuring the reentrant nature of Starlake’s API.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-next-journey">The next journey<a href="#the-next-journey" class="hash-link" aria-label="Direct link to The next journey" title="Direct link to The next journey">​</a></h2>
<p>Today with hundreds of gigabytes of data loaded and transformed daily into thousands of tables in various data warehouses, we can confidently say that Starlake is battle tested and ready for the most demanding data engineering &amp; analytics challenges.</p>
<p>As Starlake is, and will always be open source, join us in building a supportive community. Your insights and feature requests aren’t just welcome, they guide our roadmap.</p>
<p>Get Started with Starlake:</p>
<ul>
<li>
<p>Check-out the other <a href="https://starlake-ai.github.io/starlake/" target="_blank" rel="noopener noreferrer">features</a></p>
</li>
<li>
<p>Explore our <a href="https://starlake-ai.github.io/starlake/docs/next/intro" target="_blank" rel="noopener noreferrer">documentation</a></p>
</li>
</ul>
<p>Join our community on <a href="https://github.com/starlake-ai/starlake" target="_blank" rel="noopener noreferrer">GitHub</a></p>
<p>P.S. Please star the repository: <a href="https://github.com/starlake-ai/starlake" target="_blank" rel="noopener noreferrer">https://github.com/starlake-ai/starlake</a>. Also, any issue or enhancement with Starlake, please just report it. It will fall under the scope of gracious care taking of course.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/starlake">Starlake</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/data-engineering">Data Engineering</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/starlake/blog/tags/analytics">Analytics</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer"><section class="footer-section"><div class="footer-container"><div class="footer-col"><h6 class="footer-title">STARLAKE.AI</h6></div><div class="footer-col"><h6 class="footer-title tl">Resources</h6><p class="tl"><a href="/starlake/docs/intro" class="footer-link">Docs</a></p><p class="tl"><a href="/starlake/blog" class="footer-link">Blog</a></p><div class="footer-social-icons tl"><a class="footer-link"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="me-2" height="24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a class="footer-link"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="me-2" height="24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a class="footer-link"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="me-2" height="24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></div></section><div class="footer-bottom">Copyright © 2024 <!-- --> <a class="footer-link footer-link-bold" href="https://starlake.ai">Starlake.ai</a></div></footer></div>
</body>
</html>